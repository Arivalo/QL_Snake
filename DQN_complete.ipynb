{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arivalo/QL_Snake/blob/master/DQN_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee0H6toeuGfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Snake:\n",
        "    def __init__(self, board_size):\n",
        "        self.x = 8\n",
        "        self.y = 7\n",
        "        self.size = 4\n",
        "        self.dir = 1  # directions are 0-up, 1-right, 2-down, 3-left\n",
        "        self.ate = False\n",
        "        self.crashed = False\n",
        "        self.board_size = board_size\n",
        "        self.body = []\n",
        "        self.color = (255, 255, 255)\n",
        "        for i in range(self.size):\n",
        "            self.body.append((self.x-i, self.y-i))\n",
        "\n",
        "    def move(self):\n",
        "        if self.dir % 2 == 0:\n",
        "            if self.dir > 0:\n",
        "                self.y += 1\n",
        "            else:\n",
        "                self.y -= 1\n",
        "        else:\n",
        "            if self.dir > 1:\n",
        "                self.x += 1\n",
        "            else:\n",
        "                self.x -= 1\n",
        "\n",
        "        if self.x >= self.board_size:  # no walls, snake will go through\n",
        "            self.x = 0\n",
        "            self.crashed = True\n",
        "        elif self.x < 0:\n",
        "            self.x = self.board_size - 1\n",
        "            self.crashed = True\n",
        "        if self.y >= self.board_size:\n",
        "            self.y = 0\n",
        "            self.crashed = True\n",
        "        elif self.y < 0:\n",
        "            self.y = self.board_size - 1\n",
        "            self.crashed = True\n",
        "\n",
        "        if self.ate:\n",
        "            self.ate = False\n",
        "            self.body.append(self.body[-1])\n",
        "\n",
        "        self.body[0] = (self.x, self.y)\n",
        "        for i in range(len(self.body)):\n",
        "            if i > 0:\n",
        "                self.body[-i] = self.body[-i-1]\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        return self.x - other.x, self.y - other.y\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.x == other.x and self.y == other.y\n",
        "\n",
        "    def change_direction(self, new_dir):\n",
        "        if abs(self.dir - new_dir) != 2:\n",
        "            self.dir = new_dir\n",
        "\n",
        "    def act(self, new_dir):\n",
        "        if new_dir != self.dir:\n",
        "            self.change_direction(new_dir)\n",
        "        self.move()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeJdycvkvVv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Treat:\n",
        "    def __init__(self, board_size):\n",
        "        self.board_size = board_size\n",
        "        self.color = (250, 150, 50)\n",
        "        self.x = np.random.randint(0, self.board_size)\n",
        "        self.y = np.random.randint(0, self.board_size)\n",
        "\n",
        "    def change_pos(self):\n",
        "        self.x = np.random.randint(0, self.board_size)\n",
        "        self.y = np.random.randint(0, self.board_size)\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        return other.x - self.x, other.y - self.y\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.x == other.x and self.y == other.y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzYNFRI-vZO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class SnakeEnvObject:\n",
        "    SW = 600\n",
        "    SIZE = 15\n",
        "    WALLS = True\n",
        "    LOSE_PENALTY = 601\n",
        "    EAT_REWARD = 60\n",
        "    MOVE_PENALTY = 3\n",
        "    ACTION_SPACE_SIZE = 4\n",
        "    OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)\n",
        "    FPS = 30\n",
        "    SCALING = SW // SIZE\n",
        "    RETURN_IMAGES = True # change depending on whether you use neural network or simple QL\n",
        "\n",
        "    def reset(self):\n",
        "        self.player = Snake(self.SIZE)\n",
        "        self.food = Treat(self.SIZE)\n",
        "        inside = False\n",
        "        for part in self.player.body:\n",
        "            if part == (self.food.x, self.food.y):\n",
        "                inside = True\n",
        "                break\n",
        "        while inside:\n",
        "            inside = False\n",
        "            self.food.change_pos()\n",
        "            for part in self.player.body:\n",
        "                if part == (self.food.x, self.food.y):\n",
        "                    inside = True\n",
        "                    break\n",
        "\n",
        "        self.episode_step = 0\n",
        "        observation = self.get_observation()\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def get_observation(self):\n",
        "\n",
        "        if self.RETURN_IMAGES:\n",
        "            observation = np.array(self.get_image())\n",
        "        else:\n",
        "            left_b = 0  # is there a part of snake or wall (if turned on) on the left\n",
        "            right_b = 0  # same on right\n",
        "            up_b = 0   # same but a grid up\n",
        "            down_b = 0  # same but a grid down\n",
        "            for part in self.player.body:\n",
        "                if part == (self.player.x - 1, self.player.y):\n",
        "                    left_b = 1\n",
        "                elif self.player.x - 1 < 0 and part == (self.SIZE-1, self.player.y):\n",
        "                    left_b = 1\n",
        "                if part == ((self.player.x + 1) % self.SIZE, self.player.y):\n",
        "                    right_b = 1\n",
        "                if part == (self.player.x, self.player.y - 1):\n",
        "                    up_b = 1\n",
        "                elif self.player.y - 1 < 0 and part == (self.player.x, self.SIZE-1):\n",
        "                    up_b = 1\n",
        "                if part == (self.player.x, (self.player.y + 1) % self.SIZE):\n",
        "                    down_b = 1\n",
        "\n",
        "            if self.WALLS:\n",
        "                if self.player.x == 0:\n",
        "                    left_b = 1\n",
        "                if self.player.x == self.SIZE-1:\n",
        "                    right_b = 1\n",
        "                if self.player.y == 0:\n",
        "                    up_b = 1\n",
        "                if self.player.x == self.SIZE-1:\n",
        "                    down_b = 1\n",
        "\n",
        "            # observation contains: tuple of relative coordinates of snake to food, walls or body parts\n",
        "            # in all directions and current snake direction\n",
        "            observation = ((self.player-self.food), left_b, right_b, up_b, down_b, self.player.dir)\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def step(self, action):\n",
        "        self.episode_step += 1\n",
        "        self.player.act(action)\n",
        "\n",
        "        new_observation = self.get_observation()\n",
        "\n",
        "        # if walls are turned on\n",
        "        if self.WALLS and self.player.crashed:\n",
        "            reward = -self.LOSE_PENALTY\n",
        "            done = True\n",
        "        # if snake is at the same grid as food\n",
        "        elif self.player == self.food:\n",
        "            self.player.ate = True\n",
        "            self.food.change_pos()\n",
        "            done = False\n",
        "            inside = False\n",
        "            for part in self.player.body:\n",
        "                if part == (self.food.x, self.food.y):\n",
        "                    inside = True\n",
        "                    break\n",
        "            while inside:\n",
        "                inside = False\n",
        "                self.food.change_pos()\n",
        "                for part in self.player.body:\n",
        "                    if part == (self.food.x, self.food.y):\n",
        "                        inside = True\n",
        "                        break\n",
        "            reward = self.EAT_REWARD\n",
        "        # otherwise its either normal move or snake hit itself\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = False\n",
        "            for part in self.player.body:\n",
        "                if part == (self.player.x, self.player.y) and part is not self.player.body[0]:\n",
        "                    done = True\n",
        "                    reward = -self.LOSE_PENALTY\n",
        "                    break\n",
        "            if reward == 0:\n",
        "                reward = -self.MOVE_PENALTY\n",
        "\n",
        "        return new_observation, reward, done\n",
        "\n",
        "    def render(self):\n",
        "        img = self.get_image()\n",
        "        img = img.resize((300, 300))\n",
        "        cv2.imshow(\"image\", np.array(img))\n",
        "        cv2.waitKey(30)\n",
        "        '''pg.init()\n",
        "        clock = pg.time.Clock()\n",
        "        win = pg.display.set_mode((self.SW, self.SW))\n",
        "        clock.tick(self.FPS)\n",
        "        win.fill((0,0,0))\n",
        "        for part in self.player.body:\n",
        "            pg.draw.rect(win, self.player.color,\n",
        "                         (part[0] * self.SCALING + 1, part[1] * self.SCALING + 1, self.SCALING - 2, self.SCALING - 2))\n",
        "        pg.draw.rect(win, self.food.color,\n",
        "                     (self.food.x * self.SCALING, self.food.y * self.SCALING, self.SCALING, self.SCALING))\n",
        "        pg.display.update()'''\n",
        "\n",
        "    def get_image(self):\n",
        "        env = np.zeros((self.SIZE, self.SIZE, 3), dtype=np.uint8)\n",
        "        for part in self.player.body:\n",
        "            env[part[0]][part[1]] = self.player.color\n",
        "        env[self.player.x][self.player.y] = (100, 200, 200)\n",
        "        env[self.food.x][self.food.y] = self.food.color\n",
        "        img = Image.fromarray(env, 'RGB')\n",
        "        return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFStSCpQveIG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "9c30813d-a4bf-47e6-b54d-dd7451f10405"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.optimizers import Adam\n",
        "from collections import deque\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "\n",
        "MODEL_NAME = \"256x2\"\n",
        "# nn updates settings\n",
        "MIN_REPLAY_MEMORY_SIZE = 1_000\n",
        "REPLAY_MEMORY_SIZE = 50_000\n",
        "MINIBATCH_SIZE = 64\n",
        "DISCOUNT = 0.99\n",
        "UPDATE_TARGET_EVERY = 5\n",
        "MIN_REWARD = -200\n",
        "\n",
        "EPISODES = 20_000\n",
        "\n",
        "# exploration\n",
        "MAX_STEPS = 200\n",
        "epsilon = 1\n",
        "EPSILON_DECAY = 0.99975\n",
        "MIN_EPSILON = 0.001\n",
        "\n",
        "# stats\n",
        "AGGREGATE_STATS_EVERY = 50\n",
        "SHOW_PREVIEW = False\n",
        "\n",
        "# inits\n",
        "env = SnakeEnvObject()\n",
        "ep_rewards = [-200]\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "tf.set_random_seed(1)\n",
        "\n",
        "if not os.path.isdir('DQN_SNAKE_models'):\n",
        "    os.makedirs('DQN_SNAKE_models')\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self):\n",
        "        self.model = self.create_model()\n",
        "        self.target_model = self.create_model()\n",
        "        self.target_model.set_weights(self.target_model.get_weights())\n",
        "\n",
        "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
        "        self.tensorboard = ModifiedTensorBoard(log_dir=f\"logs/{MODEL_NAME}-{time.time()}\")\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "    def create_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(256, (3, 3), input_shape=env.OBSERVATION_SPACE_VALUES))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D(2,2))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3)))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(MaxPooling2D(2,2))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(256))\n",
        "        model.add(Dense(env.ACTION_SPACE_SIZE, activation=\"linear\"))\n",
        "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def update_replay_memory(self, transition):\n",
        "        self.replay_memory.append(transition)\n",
        "\n",
        "    def get_qs(self, state):\n",
        "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n",
        "\n",
        "    def train(self, terminal_state, step):\n",
        "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
        "        current_states = np.array([transition[0] for transition in minibatch])/255\n",
        "        current_qs_list = self.model.predict(current_states)\n",
        "\n",
        "        new_current_states = np.array([transition[3] for transition in minibatch])/255\n",
        "        future_qs_list = self.target_model.predict(new_current_states)\n",
        "\n",
        "        X = []\n",
        "        y = []\n",
        "\n",
        "        for index, (current_state, action, reward, new_current_states, done) in enumerate(minibatch):\n",
        "            if not done:\n",
        "                max_future_q = np.max(future_qs_list[index])\n",
        "                new_q = reward + DISCOUNT * max_future_q\n",
        "            else:\n",
        "                new_q = reward\n",
        "\n",
        "            current_qs = current_qs_list[index]\n",
        "            current_qs[action] = new_q\n",
        "\n",
        "            X.append(current_state)\n",
        "            y.append(current_qs)\n",
        "\n",
        "        self.model.fit(np.array(X)/255, np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False,\n",
        "                       callbacks=[self.tensorboard] if terminal_state else None)\n",
        "\n",
        "        if terminal_state:\n",
        "            self.target_update_counter += 1\n",
        "\n",
        "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "            self.target_update_counter = 0\n",
        "\n",
        "\n",
        "# MODIFIED TENSORBOARD CLASS TO GET ONLY ONE LOG; CREDITS TO PYTHONPROGRAMMING.NET\n",
        "class ModifiedTensorBoard(TensorBoard):\n",
        "\n",
        "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.step = 1\n",
        "        self.writer = tf.summary.FileWriter(self.log_dir)\n",
        "\n",
        "    # Overriding this method to stop creating default log writer\n",
        "    def set_model(self, model):\n",
        "        pass\n",
        "\n",
        "    # Overrided, saves logs with our step number\n",
        "    # (otherwise every .fit() will start writing from 0th step)\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.update_stats(**logs)\n",
        "\n",
        "    # Overrided\n",
        "    # We train for one batch only, no need to save anything at epoch end\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        pass\n",
        "\n",
        "    # Overrided, so won't close writer\n",
        "    def on_train_end(self, _):\n",
        "        pass\n",
        "\n",
        "    # Custom method for saving own metrics\n",
        "    # Creates writer, writes custom metrics and closes writer\n",
        "    def update_stats(self, **stats):\n",
        "        self._write_logs(stats, self.step)\n",
        "\n",
        "\n",
        "agent = DQNAgent()\n",
        "\n",
        "for episode in tqdm(range(1, EPISODES+1), ascii=True, unit=\"episode\"):\n",
        "    agent.tensorboard.step = episode\n",
        "\n",
        "    episode_reward = 0\n",
        "    step = 1\n",
        "    current_state = env.reset()\n",
        "\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if np.random.random() > epsilon:\n",
        "            action = np.argmax(agent.get_qs(current_state))\n",
        "        else:\n",
        "            action = np.random.randint(0, env.ACTION_SPACE_SIZE)\n",
        "\n",
        "        new_state, reward, done = env.step(action)\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
        "            env.render()\n",
        "\n",
        "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
        "        agent.train(done, step)\n",
        "\n",
        "        current_state = new_state\n",
        "        step += 1\n",
        "\n",
        "        if env.episode_step > MAX_STEPS:\n",
        "            done = True\n",
        "\n",
        "    ep_rewards.append(episode_reward)\n",
        "\n",
        "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
        "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:]) / len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "        agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward,\n",
        "                                       epsilon=epsilon)\n",
        "\n",
        "        if min_reward >= MIN_REWARD:\n",
        "            agent.model.save(f'models/{MODEL_NAME}_{max_reward:_>7.2f}')\n",
        "\n",
        "    if epsilon > MIN_EPSILON:\n",
        "        epsilon *= EPSILON_DECAY\n",
        "        epsilon = max(MIN_EPSILON, epsilon)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0814 15:32:50.183879 140379236976512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0814 15:32:50.186017 140379236976512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0814 15:32:50.196222 140379236976512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0814 15:32:50.225148 140379236976512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0814 15:32:50.227939 140379236976512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0814 15:32:50.236781 140379236976512 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0814 15:32:50.325902 140379236976512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0814 15:32:50.555368 140379236976512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "  7%|6         | 1305/20000 [09:25<2:47:26,  1.86episode/s]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}